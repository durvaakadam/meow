{
  "file_path": "uploads/1763355278492.pdf",
  "filename": "AIML_HA1_Iris.pdf",
  "mime_type": "application/pdf",
  "chunks": [
    "AIML_HA1\nNAME: SHRUTI KADAM ROLL NO: XX\nAssignment 1: Demonstration of Grid Search and Random Search Hyperparameter Tuning\n1. Dataset Selection\n(cid:212) The Iris dataset from scikit-learn was chosen for this experiment.\n(cid:212) It consists of 150 samples, each described by 4 features (sepal length, sepal width, petal length,\npetal width).\n(cid:212) The target variable represents three different classes of Iris flowers.\n2. Model Selection\n(cid:212) Logistic Regression was chosen as the classification algorithm.\n(cid:212) This model is widely used for multiclass problems, is interpretable, and has several\nhyperparameters that significantly affect performance.\n(cid:212) The hyperparameters considered for tuning were:\nl C: Regularization strength (inverse of penalty).\nl Penalty: Type of regularization (L1 or L2).\nl Solver: Optimization algorithm used to fit the model.\nl Multi-class strategy: One-vs-Rest (OvR) or Multinomial.\n3. Grid Search Hyperparameter Tuning\n(cid:212) Grid Search was",
    " applied to exhaustively evaluate all possible combinations of selected\nhyperparameters.\n(cid:212) Five-fold cross-validation was used to assess model performance for each combination.\n(cid:212) The best parameter set was identified as: C = 10, penalty = L2, solver = lbfgs, multi_class =\nMultinomial.\n(cid:212) The corresponding best cross-validation score was 98.3%.\n4. Random Search Hyperparameter Tuning\n(cid:212) Random Search was applied to sample a limited number of hyperparameter combinations from\npredefined ranges.\n(cid:212) Twenty random combinations were tested using five-fold cross-validation.\n(cid:212) The best parameter set was identified as:\nu C » 3.47, penalty = L2, solver = saga, multi_class = OvR.\n(cid:212) The corresponding best cross-validation score was 97.0%.\n5. Results Comparison\nl Grid Search achieved slightly higher performance (98.3%) compared to Random Search\n(97.0%).\nl Grid Search was more exhaustive but computationally more expensive.\nl Random Search was faster",
    " and more efficient, but less thorough.\nl Both approaches produced highly accurate models, confirming the effectiveness of Logistic\nRegression on this dataset.\n6. Conclusion\n(cid:212) Grid Search: Achieved a CV score of 98.3% but a test accuracy of ~96.7%, showing strong\nperformance.\n(cid:212) Random Search: Achieved a CV score of 97.0% but test accuracy of ~95.6%, indicating good\ngeneralization to unseen data.\n(cid:212) Conclusion: Exhaustive hyperparameter search (Grid Search) does not always guarantee the\nbest test performance. Random Search can be more efficient and may generalize better in practice.\nCode Implementation:\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV,\nRandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n# Load dataset\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_",
    "size=0.3,\nrandom_state=42)\n# Model\nmodel = LogisticRegression(max_iter=5000)\n# Grid Search\nparam_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'],\n'solver': ['liblinear', 'saga', 'lbfgs'],\n'multi_class': ['ovr', 'multinomial']}\ngrid = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\ngrid.fit(X_train, y_train)\nprint(\"Best Grid Search Params:\", grid.best_params_)\nprint(\"Best Grid CV Score:\", grid.best_score_)\n# Random Search\nparam_dist = {'C': np.logspace(-3, 3, 100),\n'penalty': ['l1', 'l2'],\n'solver': ['saga'],\n'multi_class': ['ovr', 'multinomial']}\nrandom = RandomizedSearchCV(model, param_dist, n_iter=20, cv=5,\nrandom_state=42, n_jobs=-1)\nrandom.fit(X_train, y_train)\nprint(\"Best Random Search Params:\", random.best_params_)\nprint(\"Best Random CV Score:\", random.best_score_)\n# Test Evaluation\ny_pred_grid = grid.predict(X_test)\ny_pred_random = random.predict(X_test)\nprint(\"Grid Search Test Accuracy:\", accuracy_score(y_test, y_pred_grid))\nprint(\"Random Search Test Accuracy:\", accuracy_sc",
    "ore(y_test,\ny_pred_random))\n"
  ],
  "full_text_preview": "AIML_HA1\nNAME: SHRUTI KADAM ROLL NO: XX\nAssignment 1: Demonstration of Grid Search and Random Search Hyperparameter Tuning\n1. Dataset Selection\n(cid:212) The Iris dataset from scikit-learn was chosen for this experiment.\n(cid:212) It consists of 150 samples, each described by 4 features (sepal length, sepal width, petal length,\npetal width).\n(cid:212) The target variable represents three different classes of Iris flowers.\n2. Model Selection\n(cid:212) Logistic Regression was chosen as the classification algorithm.\n(cid:212) This model is widely used for multiclass problems, is interpretable, and has several\nhyperparameters that significantly affect performance.\n(cid:212) The hyperparameters considered for tuning were:\nl C: Regularization strength (inverse of penalty).\nl Penalty: Type of regularization (L1 or L2).\nl Solver: Optimization algorithm used to fit the model.\nl Multi-class strategy: One-vs-Rest (OvR) or Multinomial.\n3. Grid Search Hyperparameter Tuning\n(cid:212) Grid Search was applied to exhaustively evaluate all possible combinations of selected\nhyperparameters.\n(cid:212) Five-fold cross-validation was used to assess model performance for each combination.\n(cid:212) The best parameter set was identified as: C = 10, penalty = L2, solver = lbfgs, multi_class =\nMultinomial.\n(cid:212) The corresponding best cross-validation score was 98.3%.\n4. Random Search Hyperparameter Tuning\n(cid:212) Random Search was applied to sample a limited number of hyperparameter combinations from\npredefined ranges.\n(cid:212) Twenty random combinations were tested using five-fold cross-validation.\n(cid:212) The best parameter set was identified as:\nu C » 3.47, penalty = L2, solver = saga, multi_class = OvR.\n(cid:212) The corresponding best cross-validation score was 97.0%.\n5. Results Comparison\nl Grid Search achieved slightly higher performance (98.3%) compared to Random Search\n(97.0%).\nl Grid Search was more exhaustive but computationally more expensive.\nl Random Search was faster and more efficient, but less thorough.\nl Both approaches produced highly accurate models, confirming the effectiveness of Logistic\nRegression on this dataset.\n6. Conclusion\n(cid:212) Grid Search: Achieved a CV score of 98.3% but a test accuracy of ~96.7%, showing strong\nperformance.\n(cid:212) Random Search: Achieved a CV score of 97.0% but test accuracy of ~95.6%, indicating good\ngeneralization to unseen data.\n(cid:212) Conclusion: Exhaustive hyperparameter search (Grid Search) does not always guarantee the\nbest test performance. Random Search can be more efficient and may generalize better in practice.\nCode Implementation:\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV,\nRandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n# Load dataset\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\nrandom_state=42)\n# Model\nmodel = LogisticRegression(max_iter=5000)\n# Grid Search\nparam_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'],\n'solver': ['liblinear', 'saga', 'lbfgs'],\n'multi_class': ['ovr', 'multinomial']}\ngrid = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\ngrid.fit(X_train, y_train)\nprint(\"Best Grid Search Params:\", grid.best_params_)\nprint(\"Best Grid CV Score:\", grid.best_score_)\n# Random Search\nparam_dist = {'C': np.logspace(-3, 3, 100),\n'penalty': ['l1', 'l2'],\n'solver': ['saga'],\n'multi_class': ['ovr', 'multinomial']}\nrandom = RandomizedSearchCV(model, param_dist, n_iter=20, cv=5,\nrandom_state=42, n_jobs=-1)\nrandom.fit(X_train, y_train)\nprint(\"Best Random Search Params:\", random.best_params_)\nprint(\"Best Random CV Score:\", random.best_score_)\n# Test Evaluation\ny_pred_grid = grid.predict(X_test)\ny_pred_random = random.predict(X_test)\nprint(\"Grid Search Test Accuracy:\", accuracy_score(y_test, y_pred_grid))\nprint(\"Random Search Test Accuracy:\", accuracy_score(y_test,\ny_pred_random))\n"
}