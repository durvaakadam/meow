{
  "file_path": "uploads/1763400179816.pdf",
  "filename": "Experiment 5.pdf",
  "mime_type": "application/pdf",
  "chunks": [
    "Name: Sachi Anant Jadhav Roll. No. 22 St. Francis Institute of Technology, Mumbai-400 103 Department Of Information Technology A.Y. 2025-2026 Class: BE-ITA/B, Semester: VII Subject: Data Science Lab Experiment – 5 1. Aim: To design a simple LSTM Network. 2. Objectives: Students should be familiarize with Learning Architectures and Frameworks using LSTM and CNN. 3. Prerequisite: Python basics 4. Pre-Experiment Exercise: Theory: Long Short Term Memory Network(LSTM). In concept, an LSTM recurrent unit tries to “remember” all the past knowledge that the network is seen so far and to “forget” irrelevant data. This is done by introducing different activation function layers called “gates” for different purposes. Each LSTM recurrent unit also maintains a vector called the Internal Cell State which conceptually describes the information that was chosen to be retained by the previous LSTM recurrent unit. A Long Short Term Memory Network consists of four different gates for different purposes as",
    " described below:- Forget Gate(f): It determines to what extent to forget the previous data. Input Gate(i): It determines the extent of information be written onto the Internal Cell State. Input Modulation Gate(g): It is often considered as a sub-part of the input gate and much literature on LSTM’s does not even mention it and assume it is inside the Input gate. It is used to modulate the information that the Input gate will write onto the Internal State Cell by adding non-linearity to the information and making the information Zero-mean. This is done to reduce the learning time as Zero-mean input has faster convergence. Although this gate’s actions are less important than the others and are often treated as a finesse-providing concept, it is good practice to include this gate in the structure of the LSTM unit. Output Gate(o): It determines what output(next Hidden State) to generate from the current Internal Cell State. Fig:6.1 LSTM Network CNN: A convolutional neural network (CNN, or ",
    "ConvNet) is a class of artificial neural network (ANN), most commonly applied to analyze visual imagery. CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the down sampling operation they apply to the input. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain–computer interfaces, and financial time series. 6. Laboratory Exercise Procedure ● Take input the current input, the previous hidden state, and the previous internal cell state. ● Calculate the values of the four different gates by following the below steps:- ● For each gate, calculate the para",
    "meterized vectors for the current input and the previous hidden state by element-wise multiplication with the concerned vector with the respective weights for each gate. ● Apply the respective activation function for each gate element-wise on the parameterized vectors. ● Calculate the current internal cell state by first calculating the element-wise multiplication vector of the input gate and the input modulation gate, then calculate the element-wise multiplication vector of the forget gate and the previous internal cell state and then adding the two vectors. ● c_{t} = i\\odot g + f\\odot c_{t-1} Post-Experiments Exercise: A. Extended Theory: a. Explain Architecture of CNN. B. Post Lab Program: a. Write a Python program to demonstrate any real life application using CNN . C. Conclusion: 1. Write what was performed in the program (s) . 2. What is the significance of program and what Objective is achieved? D. References: [1] https://stackabuse.com/image-recognition-in-python-with-tensorflo",
    "w-and-keras/ [2] . https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn 7. Laboratory Exercise 1. Import the required libraries 2. Import the training dataset 3. Perform feature scaling to transform the data 4. Create a data structure with 60-time steps and 1 output 5. Import Keras library and its packages 6. Initialize the RNN 7. Add the LSTM layers and some dropout regularization. 8. Add the output layer. 9. Compile the RNN 10. Fit the RNN to the training set 11. Load the stock price test data for 2017 12. Get the predicted stock price for 2017 13. Visualize the results of predicted and real stock price Post Experiment Exercise: A. Write a Python program to demonstrate any real life application using CNN . # CNN for MNIST Handwritten Digit Recognition import tensorflow as tf from tensorflow.keras import datasets, layers, models import matplotlib.pyplot as plt import numpy as np # 1. Load dataset (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() # Normalize",
    " images (0–255 -> 0–1) and reshape for CNN input x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)) / 255.0 x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)) / 255.0 # 2. Build CNN model model = models.Sequential([ layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), layers.MaxPooling2D((2, 2)), layers.Conv2D(64, (3, 3), activation='relu'), layers.MaxPooling2D((2, 2)), layers.Flatten(), layers.Dense(128, activation='relu'), layers.Dense(10, activation='softmax') # 10 output classes (digits 0–9) ]) # 3. Compile model model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'] ) # 4. Train model history = model.fit( x_train, y_train, epochs=5, validation_data=(x_test, y_test) ) # 5. Evaluate test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2) print(f\"\\n✅ Test accuracy: {test_acc:.4f}\") # 6. Predict on test samples predictions = model.predict(x_test[:5]) predicted_labels = np.argmax(predictions, axis=1) # Show first",
    " 5 test images with predictions plt.figure(figsize=(10, 5)) for i in range(5): plt.subplot(1, 5, i + 1) plt.imshow(x_test[i].reshape(28, 28), cmap=\"gray\") plt.title(f\"Pred: {predicted_labels[i]}\\nTrue: {y_test[i]}\") plt.axis(\"off\") plt.show() Output:"
  ],
  "full_text_preview": "Name: Sachi Anant Jadhav Roll. No. 22 St. Francis Institute of Technology, Mumbai-400 103 Department Of Information Technology A.Y. 2025-2026 Class: BE-ITA/B, Semester: VII Subject: Data Science Lab Experiment – 5 1. Aim: To design a simple LSTM Network. 2. Objectives: Students should be familiarize with Learning Architectures and Frameworks using LSTM and CNN. 3. Prerequisite: Python basics 4. Pre-Experiment Exercise: Theory: Long Short Term Memory Network(LSTM). In concept, an LSTM recurrent unit tries to “remember” all the past knowledge that the network is seen so far and to “forget” irrelevant data. This is done by introducing different activation function layers called “gates” for different purposes. Each LSTM recurrent unit also maintains a vector called the Internal Cell State which conceptually describes the information that was chosen to be retained by the previous LSTM recurrent unit. A Long Short Term Memory Network consists of four different gates for different purposes as described below:- Forget Gate(f): It determines to what extent to forget the previous data. Input Gate(i): It determines the extent of information be written onto the Internal Cell State. Input Modulation Gate(g): It is often considered as a sub-part of the input gate and much literature on LSTM’s does not even mention it and assume it is inside the Input gate. It is used to modulate the information that the Input gate will write onto the Internal State Cell by adding non-linearity to the information and making the information Zero-mean. This is done to reduce the learning time as Zero-mean input has faster convergence. Although this gate’s actions are less important than the others and are often treated as a finesse-providing concept, it is good practice to include this gate in the structure of the LSTM unit. Output Gate(o): It determines what output(next Hidden State) to generate from the current Internal Cell State. Fig:6.1 LSTM Network CNN: A convolutional neural network (CNN, or ConvNet) is a class of artificial neural network (ANN), most commonly applied to analyze visual imagery. CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the down sampling operation they apply to the input. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain–computer interfaces, and financial time series. 6. Laboratory Exercise Procedure ● Take input the current input, the previous hidden state, and the previous internal cell state. ● Calculate the values of the four different gates by following the below steps:- ● For each gate, calculate the parameterized vectors for the current input and the previous hidden state by element-wise multiplication with the concerned vector with the respective weights for each gate. ● Apply the respective activation function for each gate element-wise on the parameterized vectors. ● Calculate the current internal cell state by first calculating the element-wise multiplication vector of the input gate and the input modulation gate, then calculate the element-wise multiplication vector of the forget gate and the previous internal cell state and then adding the two vectors. ● c_{t} = i\\odot g + f\\odot c_{t-1} Post-Experiments Exercise: A. Extended Theory: a. Explain Architecture of CNN. B. Post Lab Program: a. Write a Python program to demonstrate any real life application using CNN . C. Conclusion: 1. Write what was performed in the program (s) . 2. What is the significance of program and what Objective is achieved? D. References: [1] https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/ [2] . https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn 7. Laboratory Exercise 1. Import the required libraries 2. Import the training dataset 3. Perform feature scaling to transform the data 4. Create a data structure with 60-time steps and 1 output 5. Import Keras library and its packages 6. Initialize the RNN 7. Add the LSTM layers and some dropout regularization. 8. Add the output layer. 9. Compile the RNN 10. Fit the RNN to the training set 11. Load the stock price test data for 2017 12. Get the predicted stock price for 2017 13. Visualize the results of predicted and real stock price Post Experiment Exercise: A. Write a Python program to demonstrate any real life application using CNN . # CNN for MNIST Handwritten Digit Recognition import tensorflow as tf from tensorflow.keras import datasets, layers, models import matplotlib.pyplot as plt import numpy as np # 1. Load dataset (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() # Normalize images (0–255 -> 0–1) and reshape for CNN input x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)) / 255.0 x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)) / 255.0 # 2. Build CNN model model = models.Sequential([ layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), layers.MaxPooling2D((2, 2)), layers.Conv2D(64, (3, 3), activation='relu'), layers.MaxPooling2D((2, 2)), layers.Flatten(), layers.Dense(128, activation='relu'), layers.Dense(10, activation='softmax') # 10 output classes (digits 0–9) ]) # 3. Compile model model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'] ) # 4. Train model history = model.fit( x_train, y_train, epochs=5, validation_data=(x_test, y_test) ) # 5. Evaluate test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2) print(f\"\\n✅ Test accuracy: {test_acc:.4f}\") # 6. Predict on test samples predictions = model.predict(x_test[:5]) predicted_labels = np.argmax(predictions, axis=1) # Show first 5 test images with predictions plt.figure(figsize=(10, 5)) for i in range(5): plt.subplot(1, 5, i + 1) plt.imshow(x_test[i].reshape(28, 28), cmap=\"gray\") plt.title(f\"Pred: {predicted_labels[i]}\\nTrue: {y_test[i]}\") plt.axis(\"off\") plt.show() Output:"
}